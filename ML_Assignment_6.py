# -*- coding: utf-8 -*-
"""Group_2_ML_Assignment_6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16jmkDIgMLAdPukwunVnHIGLHRl_YkQvI

**Installing lime and shap**
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install lime shap

"""**Importing libraries**"""

#Importing necessary libraries and functions
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.svm import SVC
from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
import lime
import lime.lime_tabular
import shap

pd.set_option('display.max_rows', None)

"""**Loading the dataset into dataframe**

"""

# Loading the dataset and displaying first 5 elements of the data frame
#df = pd.read_csv("bank_data_train.csv")
#df.head()

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('./bank_data_train.csv')

df.head()

"""**Displaying the shape(row, column count) of the dataframe**"""

df.shape

"""**Central tendency and other properties of the data frame**"""

df.describe(include='all')

"""# Data Cleaning

**Handling null values**
"""

#screening the columns containing values only with 'NaN' and '0'
nan_values = df.columns[((df == 0) | df.isnull()).all()]
nan_values.values

"""**Dropping columns with null values**"""

#Dropping the columns with 'NaN' and '0' value
df = df.drop(columns=nan_values)
df.shape

df = df.sample(n=20000, random_state=42)

"""**Splitting the data into Train and Test data**"""

# Split the data
X = df.drop('TARGET', axis = 1)
y = df['TARGET']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

"""**Shape of Train and Test data**"""

print(X_train.shape)
print(X_test.shape)

"""**Handling the duplicate values for Train and Test data**"""

print(X_train.shape)
duplicates_train = X_train[X_train.duplicated()]
print(duplicates_train.shape)

print(X_test.shape)
duplicates_test = X_test[X_test.duplicated()]
print(duplicates_test.shape)

"""**Checking unique values of features with data type 'object' to compute values for One Hot Encoding**"""

df['CLNT_TRUST_RELATION'].unique()

df['CLNT_JOB_POSITION'].unique()

"""**Analyzing the values of categorical data to check if any value can be changed**"""

# Creating a dictionary with the russian words to translate the values to english words
russiantoenglish = {
    "MOTHER": "mother",
    "BROTHER": "brother",
    "FRIEND": "friend",
    "SISTER": "sister",
    "OTHER": "other",
    "RELATIVE": "relative",
    "DAUGHTER": "daughter",
    "SON": "son",
    "FATHER": "father",
    "Близкий ро": "close ro",
    "Друг": "friend",
    "Отец": "father",
    "Сестра": "sister",
    "Сын": "son",
    "Мать": "mother",
    "Муж": "husband",
    "Брат": "brother",
    "мать": "mother",
    "Дальний ро": "far ro",
    "Дочь": "daughter",
    "Жена": "wife",
    "начальник отдела": "department head",
    "ГЕНЕРАЛЬНЫЙ ДИРЕКТОР": "ceo",
    "Эксперт контакт Центра": "expert contact center",
    "ПРОХЛДЧИК": "cool",
    "ХУДОЖНИК КОМПЬЮТЕРНОЙ ГРАФИКИ": "computer graphics artist",
    'ВЕДУЩИЙ ЭКОНОМИСТ': 'senior economist',
    'ДИРЕКТОР': 'director',
    'Начальник Управления': 'department head',
    'Торговывы представитель': 'sales representative',
    'ФЕЛЬДШЕР УЧРЕЖДЕНИЯ ФКУ ИК-3 УФСИН': 'paramedic at FKU IK-3 FSIN',
    'Руководитель отдела логистики': 'logistics department head'
}

# Defining a function called russian to convert russian words to english words
def russian(data, translate_columns, russiantoenglish):
    replaced = data.copy()
    for col in translate_columns:
        if col in replaced.columns:
            # Replace Russian words with English in the specified column
            replaced[col] = replaced[col].map(russiantoenglish).fillna(replaced[col])
            # print(replaced[col])
        else:
            print(f"Column '{col}' does not exist in the data.")
    return replaced

# List of columns which should considered for translation
translate_coloums = ['CLNT_TRUST_RELATION', 'CLNT_JOB_POSITION']

# Changing the values of columns for train and test data
X_train = russian(X_train, translate_coloums, russiantoenglish)
X_test = russian(X_test, translate_coloums, russiantoenglish)

"""**Checking if the data is changed to respective output or not**"""

X_train['CLNT_TRUST_RELATION'].unique()

X_test['CLNT_TRUST_RELATION'].unique()

X_train['CLNT_JOB_POSITION'].unique()

X_test['CLNT_JOB_POSITION'].unique()

"""**Computing the sum of null values for every features in the data frame**"""

null_counts = df.isnull().sum()
print(null_counts)

# Computing the sum of values of features of train data
null_counts_train = X_train.isnull().sum()
print(null_counts_train)

# Computing the sum of values of features of test data
null_counts_test = X_test.isnull().sum()
print(null_counts_test)

"""# Feature Dropping

**Dropping Columns with Highest Percentage of Missing Values**
"""

# Defining a function to drop the null columns which is greater than the assigned threshold value
def drop_null_columns(data, null_value_threshold=200000):
  null_counts = data.isnull().sum()
  print(null_counts)
  columns_to_drop = null_counts[null_counts > null_value_threshold].index.tolist()
  print(columns_to_drop)
  return data.drop(columns_to_drop, axis=1)

# Computing the above function for train data
X_train = drop_null_columns(X_train)
print(X_train.shape)

# Computing the above function for test data
X_test = drop_null_columns(X_test)
print(X_test.shape)

"""**Dropping columns that does not provide much information**"""

X_train.drop(['ID','CLNT_JOB_POSITION'], axis=1, inplace=True)
X_test.drop(['ID','CLNT_JOB_POSITION'], axis=1, inplace=True)
print(X_train.info())
print(X_test.info())

"""**Filling Missing Values**"""

# Fill remaining missing values with appropriate statistics
for column in X_train.columns:
    if X_train[column].dtype == 'object':
        #X_train[column].fillna(X_train[column].mode()[0], inplace=True)
        X_train[column].fillna('None', inplace=True)
    else:
        X_train[column].fillna(X_train[column].mean(), inplace=True)

for column in X_test.columns:
    if X_test[column].dtype == 'object':
        #X_test[column].fillna(X_test[column].mode()[0], inplace=True)
        X_test[column].fillna('None', inplace=True)
    else:
        X_test[column].fillna(X_test[column].mean(), inplace=True)

# Displaying ten elements of the feature 'PACK'
X_train['PACK'].head(10)

"""# EDA (Exploratory Data Analysis)

**Getting the numerical and categorical features for both train and test data**
"""

numerical_train_df = X_train.select_dtypes(include=['int64', 'float64'])
categorical_train_df = X_train.select_dtypes(include=['object'])

numerical_test_df = X_test.select_dtypes(include=['int64', 'float64'])
categorical_test_df = X_test.select_dtypes(include=['object'])

"""**Displaying the numerical and categorical features of the training data**"""

print(numerical_train_df.columns)
print(categorical_train_df.columns)

"""**Computing the skewness of the numerical feature of training data**

**Skewness**
"""

numerical_train_df.select_dtypes(include=['int64', 'float64']).skew()

"""# Univariate Analysis

**Central Tendency**
"""

numerical_train_df.describe(include='all')

"""**Histogram of distribution**"""

fig, axes = plt.subplots(nrows=5, ncols=8, figsize=(30, 20))
fig.subplots_adjust(hspace=0.5, wspace=0.5)
ncolumns = numerical_train_df.columns
k=0
for i in range(5):
  for j in range(8):
    if k < len(ncolumns):
      sns.histplot(data=df, x=ncolumns[k], ax=axes[i][j])
      k+=1
plt.show()

"""# Bivariate Analysis

**Correlation Heatmap**
"""

plt.figure(figsize=(30, 20))
sns.heatmap(numerical_train_df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

"""**Box plot**"""

plt.figure(figsize=(10, 8))
sns.boxplot(data=numerical_train_df, orient='h')
plt.title('Box Plot')
plt.show()

col = numerical_train_df.columns

# Defining the number of columns per row for the plot
num_columns = 3
num_rows = (len(col) // num_columns) + (len(col) % num_columns > 0)
# Defining the figure characteristics and axes for the box plot
fig, axes = plt.subplots(nrows=num_rows, ncols=num_columns, figsize=(15, num_rows * 5))
axes = axes.flatten()

for i, col in enumerate(col):
      sns.boxplot(data=numerical_train_df, y=col, ax=axes[i])
      axes[i].set_title(f'Box Plot of {col}')
      axes[i].set_xlabel(col)
      axes[i].set_ylabel('')
# Displaying the box plot
plt.tight_layout()
plt.show()

numerical_train_df.hist(bins=50, figsize=(20, 15))
plt.show()

"""**Summary (EDA)**

# Data Preprocessing

**Getting the numerical and categorical features of training data for data preprocessing**
"""

# Numerical features of training data
numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns

# Categorical features of training data
categorical_features = X_train.select_dtypes(include=['object']).columns

# Function to print class imbalance ratio
def print_imbalance(y):
    class_counts = np.bincount(y)
    imbalance_ratio = class_counts[1] / class_counts[0]
    print(f"Imbalance ratio: {imbalance_ratio:.4f}")
    return y

def imbalance_scorer(y_true, y_pred):
    return imbalance_ratio(y_true)

# Preprocessing for numerical data of training data using Simple Imputer and Standard Scaler
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Preprocessing for categorical data of training data using Simple Imputer and One Hot Encoder
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# K-Fold Cross-Validation setup
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer

# Create and train the logistic regression model within a pipeline
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000))
])

# Handle NaN values in your target variable 'y'
# Choose an appropriate strategy based on your data and problem:

# 1. Remove rows with NaN in 'y' (if losing data is acceptable)
y_train = y_train.dropna()
X_train = X_train.loc[y_train.index]  # Adjust X accordingly if needed

# 2. Impute missing values with a suitable value (e.g., mean, median)
imputer = SimpleImputer(strategy='mean')  # Choose an appropriate strategy
y_train = imputer.fit_transform(y_train.values.reshape(-1, 1)).ravel()  # Reshape and flatten

# After handling NaN values, proceed with cross-validation:
cv_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')

print('Cross-validation scores:', cv_scores)
print('Mean cross-validation score:', cv_scores.mean())

y_train_imb = (y_train != 0).sum()/(y_train == 0).sum()
y_test_imb = (y_test != 0).sum()/(y_test == 0).sum()
print("Imbalance in Train Data: {}".format(y_train_imb))
print("Imbalance in Test Data: {}".format(y_test_imb))

# from imblearn.over_sampling import SMOTE

# # Creating the SMOTE obj with the appropriate sampling strategy
# sm = SMOTE(sampling_strategy='auto', random_state=42)

# # Applying SMOTE to training data
# X_tr_sample, y_tr_sample = sm.fit_resample(X_train, y_train)

# # Print the shapes of the resampled data
# print("X_tr_sample Dataframe Shape {}".format(X_tr_sample.shape))
# print("y_tr_sample Dataframe Shape {}".format(y_tr_sample.shape))

# # Calculating and printing imbalance ratio in resampled training data
# data_imbalance = y_tr_sample.value_counts()[1] / y_tr_sample.value_counts()[0]
# print("Imbalance in Train Data: {}".format(data_imbalance))

"""**Grid Search Cross Validation**"""

# Define the parameter grid
logi_regession_model_params = {
    'classifier__penalty': ['none', 'l2'],
    'classifier__C': np.logspace(-4, 4, 10),
    'classifier__solver': ['newton-cg'],
    'classifier__max_iter': [100]
}

# Initializing GridSearchCV
logi_regession_model = GridSearchCV(estimator=model, param_grid=logi_regession_model_params, scoring='accuracy', cv=2, verbose=True, n_jobs=-1)

"""**Logistic Regression**"""

# Fit the model
logi_regession_model.fit(X_train, y_train)

"""**Predicting Data**"""

# Generate predictions on the test data
y_pred = logi_regession_model.predict(X_test)

# Now calculate the accuracy score
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

"""**Accuracy**"""

print("Accuracy:", accuracy)

"""**Confusion Matrix**"""

print("Confusion Matrix:\n", conf_matrix)

"""**Classification Report**"""

print("Classification Report:\n", class_report)

"""**SVM (Support Vector Machine)**"""

from sklearn.svm import LinearSVC
from sklearn.decomposition import PCA

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('pca', PCA(n_components=0.95)),  # Retain 95% variance
    ('classifier', LinearSVC(class_weight='balanced', dual=False, random_state=42))
])

# Parameter grid
param_distributions = {
    'classifier__C': [0.1, 1, 10, 100],
    'classifier__loss': ['squared_hinge'],
    'classifier__max_iter': [1000, 5000, 10000]
}

# Randomized search
random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=10, cv=5, random_state=42)

random_search.fit(X_train, y_train)

"""**Best hyperparameters and scores**"""

# After fitting, you can get the best hyperparameters and the corresponding score
best_params = random_search.best_params_
best_score = random_search.best_score_

# Print the best hyperparameters and score
print(f"Best parameters: {best_params}")
print(f"Best roc_auc : {best_score:.4f}")

# Get the best estimator
best_model = random_search.best_estimator_

# Make predictions on the test set (replace X_test with your test set)

y_pred = best_model.predict(X_test)
y_pred_proba = best_model.decision_function(X_test)  # Use decision_function for LinearSVC

import numpy as np
from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt

"""**Accuracy**"""

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print(f"Accuracy: {accuracy:.4f}")

"""**Confusion Matrix**"""

print("Confusion Matrix:")
print(conf_matrix)

"""**Classification Report**"""

print("Classification Report:")
print(class_report)

"""**ROC AUC**"""

print(f"ROC AUC: {roc_auc:.4f}")

"""**ROC Curve**"""

# Plot ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='red', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='teal', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""**Confusion Matrix**"""

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""**Assignment 5**

**Logistic Regression Sensitivity Analysis**
"""

# Logistic regression model and sensitivity analysis
logistic_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LogisticRegression(max_iter=10000))])

param_distributions_logistic = {
    'classifier__penalty': ['l1', 'l2'],
    'classifier__C': np.logspace(-4, 4, 20),
    'classifier__solver': ['liblinear']
}

random_search_logistic = RandomizedSearchCV(logistic_pipeline, param_distributions_logistic, n_iter=5, cv=5, random_state=42)

random_search_logistic.fit(X_train, y_train)

best_logistic_model = random_search_logistic.best_estimator_
y_pred_logistic = best_logistic_model.predict(X_test)
logistic_accuracy = accuracy_score(y_test, y_pred_logistic)
logistic_conf_matrix = confusion_matrix(y_test, y_pred_logistic)
logistic_class_report = classification_report(y_test, y_pred_logistic)
logistic_roc_auc = roc_auc_score(y_test, y_pred_logistic)

print(f"Logistic Regression Accuracy: {logistic_accuracy:.4f}")
print("Logistic Regression Confusion Matrix:")
print(logistic_conf_matrix)
print("Logistic Regression Classification Report:")
print(logistic_class_report)
print(f"Logistic Regression ROC AUC: {logistic_roc_auc:.4f}")

"""Support Vector Machine and Sensitivity Analysis"""

# svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
#                                ('classifier', LinearSVC(class_weight='balanced', dual=False, random_state=42))])

svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', SVC(kernel='linear', probability=True))])

param_distributions_svm = {
    'classifier__C': [0.1, 1, 10, 100],
    'classifier__max_iter': [1000, 5000, 10000]
}

# param_distributions_svm = {
#     'classifier__C': [0.1, 1, 10, 100],
#     'classifier__loss': ['squared_hinge'],
#     'classifier__max_iter': [1000, 5000, 10000]
# }

# random_search_svm = RandomizedSearchCV(svm_pipeline, param_distributions_svm, n_iter=10, cv=5, random_state=42, n_jobs = -1, verbose=1)
random_search_svm = RandomizedSearchCV(svm_pipeline, param_distributions_svm, n_iter=5, cv=5, random_state=42, n_jobs = -1, verbose=1)

random_search_svm.fit(X_train, y_train)

best_svm_model = random_search_svm.best_estimator_
y_pred_svm = best_svm_model.predict(X_test)
svm_accuracy = accuracy_score(y_test, y_pred_svm)
svm_conf_matrix = confusion_matrix(y_test, y_pred_svm)
svm_class_report = classification_report(y_test, y_pred_svm)
svm_roc_auc = roc_auc_score(y_test, best_svm_model.predict_proba(X_test)[:, 1])

"""**SVM Accuracy, Confusion matrix, Classification report & ROC AUC**"""

print(f"SVM Accuracy: {svm_accuracy:.4f}")
print("SVM Confusion Matrix:")
print(svm_conf_matrix)
print("SVM Classification Report:")
print(svm_class_report)
print(f"SVM ROC AUC: {svm_roc_auc:.4f}")

import pickle
# pickle_out = open("Bank_customer_churn.pkl","wb")
# pickle.dump(best_svm_model, pickle_out)
# pickle_out.close()

"""## HTML Creation"""

!jupyter nbconvert --to html ML_Assignment_6.ipynb

from google.colab import files
files.download('ML_Assignment_6.html')

import site
print(site.getsitepackages())

!pip install Flask

# Save Logistic Regression model
with open("log_reg_model.pkl", "wb") as f:
    pickle.dump(best_logistic_model, f)

# Save SVM model
with open("svm_model.pkl", "wb") as f:
    pickle.dump(best_svm_model, f)

files.download('log_reg_model.pkl')
files.download('svm_model.pkl')

"""**Interpretation using LIME**"""

# # LIME interpretation
# explainer = lime.lime_tabular.LimeTabularExplainer(
#     training_data=np.array(X_train),
#     feature_names=X_train.columns,
#     class_names=[str(i) for i in np.unique(y_train)],
#     mode='classification'
# )

# i = 0  # index of the instance to interpret
# exp = explainer.explain_instance(X_test.iloc[i], best_svm_model.decision_function, num_features=5)
# exp.show_in_notebook(show_table=True)

# exp.as_pyplot_figure()

"""**Interpreation using SHAP**"""

# # SHAP interpretation
# explainer_shap = shap.Explainer(best_svm_model['classifier'], X_train)
# shap_values = explainer_shap(X_test)

# # Plot SHAP values for the first instance
# shap.initjs()
# shap.plots.waterfall(shap_values[0])

# # Global feature importance
# shap.plots.bar(shap_values)

"""# Summary"""

# Summary
print("From the sensitivity analysis of the Logistic Regression model, we understand the effect of each hyperparameter.")
print("LIME and SHAP interpretations help us understand local and global feature importance for the SVM model.")